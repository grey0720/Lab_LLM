{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6130ab7-3b11-4210-a161-08173ef39361",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# https://github.com/parrt/dtreeviz/issues/108\n",
    "# updated versions are needed for MLJarSupervised\n",
    "! pip3 install graphviz==0.15.0\n",
    "import graphviz\n",
    "print(graphviz.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526a306f-17ca-4129-8f91-503c600dc048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import logging\n",
    "from warnings import simplefilter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.captureWarnings(True)\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dbfb0d5-0bd1-4049-af6d-9afacaac9ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./train.csv')\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d694ac7",
   "metadata": {},
   "source": [
    "# 데이터의 방향을 확인하기 위한 그래프 -> 잠시 생략\n",
    "sns.heatmap(train_data.isnull(), cbar=False)\n",
    "sns.heatmap(test_data.isnull(), cbar=False)\n",
    "sns.swarmplot(data=train_data, x='Sex', y='Age', hue=\"Survived\")\n",
    "sns.swarmplot(data=train_data, x='Sex', y='Fare', hue=\"Survived\")\n",
    "sns.swarmplot(data=train_data, x='Sex', y='Pclass', hue=\"Survived\")\n",
    "sns.swarmplot(data=train_data, x='Sex', y='Parch', hue=\"Survived\")\n",
    "sns.swarmplot(data=train_data, x='Sex', y='SibSp', hue=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c948a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_nan_values(data, column):\n",
    "    imr = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    print(f\"Number of {column} NaN values before impute: {data[column].isnull().sum().sum()}\")\n",
    "    imr = imr.fit(data[[column]])\n",
    "    data[column] = imr.transform(data[[column]]).ravel()\n",
    "    print(f\"Number of {column} NaN values after impute: {data[column].isnull().sum().sum()}\")\n",
    "\n",
    "def remove_nan_values(data, column):\n",
    "    print(f\"Number of {column} NaN values before impute: {data[column].isnull().sum().sum()}\")\n",
    "    _data = data[data[column].notnull()]\n",
    "    print(f\"Number of {column} NaN values after impute: {_data[column].isnull().sum().sum()}\")\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7c8e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId: 0 missing values\n",
      "Survived: 0 missing values\n",
      "Pclass: 0 missing values\n",
      "Name: 0 missing values\n",
      "Sex: 0 missing values\n",
      "Age: 177 missing values\n",
      "SibSp: 0 missing values\n",
      "Parch: 0 missing values\n",
      "Ticket: 0 missing values\n",
      "Fare: 0 missing values\n",
      "Cabin: 687 missing values\n",
      "Embarked: 2 missing values\n",
      "Number of Age NaN values before impute: 177\n",
      "Number of Age NaN values after impute: 0\n",
      "Number of Embarked NaN values before impute: 2\n",
      "Number of Embarked NaN values after impute: 0\n"
     ]
    }
   ],
   "source": [
    "for column in train_data.columns:\n",
    "    print(f\"{column}: {str(sum(train_data[column].isnull()))} missing values\")\n",
    "\n",
    "impute_nan_values(train_data, 'Age')\n",
    "train_data = remove_nan_values(train_data, 'Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5960e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId: 0 missing values\n",
      "Pclass: 0 missing values\n",
      "Name: 0 missing values\n",
      "Sex: 0 missing values\n",
      "Age: 86 missing values\n",
      "SibSp: 0 missing values\n",
      "Parch: 0 missing values\n",
      "Ticket: 0 missing values\n",
      "Fare: 1 missing values\n",
      "Cabin: 327 missing values\n",
      "Embarked: 0 missing values\n",
      "Number of Age NaN values before impute: 86\n",
      "Number of Age NaN values after impute: 0\n",
      "Number of Fare NaN values before impute: 1\n",
      "Number of Fare NaN values after impute: 0\n"
     ]
    }
   ],
   "source": [
    "for column in test_data.columns:\n",
    "    print(f\"{column}: {str(sum(test_data[column].isnull()))} missing values\")\n",
    "\n",
    "impute_nan_values(test_data, 'Age')\n",
    "impute_nan_values(test_data, 'Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8910c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z score 를 이용한 outlier 제거\n",
    "def outliers_z_score(data):\n",
    "    outliers=[]\n",
    "    threshold = 6 \n",
    "    \n",
    "    mean_y = np.mean(data) #data 중앙값\n",
    "    stdev_y =np.std(data) #data 표준값\n",
    "    \n",
    "    for i in data:\n",
    "        z_score = (i-mean_y)/stdev_y \n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34055b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age Outlier: []\n",
      "Fare Outlier: [512.3292, 512.3292, 512.3292]\n",
      "Parch Outlier: [6]\n",
      "Sibsp Outlier: [8, 8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "age_outliers = outliers_z_score(train_data['Age'])\n",
    "print(f\"age Outlier: {age_outliers}\") # f -> {age_outliers}를 출력하기 위한 문자열 포멧\n",
    "for ao in age_outliers:\n",
    "    train_data = train_data[train_data.Age != ao] # \"ao\" -> 와 같은 값의 age는 삭제 -> 전체 필드를 전부 다 삭제\n",
    "\n",
    "fare_outliers = outliers_z_score(train_data['Fare'])\n",
    "print(f\"Fare Outlier: {fare_outliers}\")\n",
    "for ao in fare_outliers: # fo, so 등 많지만 그냥 ao만 사용\n",
    "    train_data = train_data[train_data.Fare != ao]\n",
    "    \n",
    "pr_outliers = outliers_z_score(train_data['Parch'])\n",
    "print(f\"Parch Outlier: {pr_outliers}\") # f -> {age_outliers}를 출력하기 위한 문자열 포멧\n",
    "for ao in pr_outliers:\n",
    "    train_data = train_data[train_data.Parch != ao]\n",
    "    \n",
    "ss_outliers = outliers_z_score(train_data['SibSp'])\n",
    "print(f\"Sibsp Outlier: {ss_outliers}\") # f -> {age_outliers}를 출력하기 위한 문자열 포멧\n",
    "for ao in ss_outliers:\n",
    "    train_data = train_data[train_data.SibSp != ao]\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9076a772",
   "metadata": {},
   "source": [
    "# 데이터의 방향을 확인하기 위한 그래프 -> 잠시 생략\n",
    "sns.heatmap(train_data.isnull(), cbar=False)\n",
    "sns.heatmap(test_data.isnull(), cbar=False)\n",
    "sns.swarmplot(data=train_data, x='Sex', y='Age', hue=\"Survived\")\n",
    "sns.swarmplot(data=train_data, x='Sex', y='Fare', hue=\"Survived\")\n",
    "sns.swarmplot(data=train_data, x='Sex', y='Pclass', hue=\"Survived\")\n",
    "sns.swarmplot(data=train_data, x='Sex', y='Parch', hue=\"Survived\")\n",
    "sns.swarmplot(data=train_data, x='Sex', y='SibSp', hue=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f610dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['Name','PassengerId','Cabin', 'Ticket'], inplace = True, axis = 1)\n",
    "test_data.drop(['Name','PassengerId','Cabin', 'Ticket'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9371b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  878 non-null    int64  \n",
      " 1   Pclass    878 non-null    int64  \n",
      " 2   Sex       878 non-null    object \n",
      " 3   Age       878 non-null    float64\n",
      " 4   SibSp     878 non-null    int64  \n",
      " 5   Parch     878 non-null    int64  \n",
      " 6   Fare      878 non-null    float64\n",
      " 7   Embarked  878 non-null    object \n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 61.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f5d511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_data[\"Embarked\"] = label_encoder.fit_transform(train_data[\"Embarked\"])\n",
    "train_data[\"Sex\"] = label_encoder.fit_transform(train_data[\"Sex\"])\n",
    "\n",
    "test_data[\"Embarked\"] = label_encoder.fit_transform(test_data[\"Embarked\"])\n",
    "test_data[\"Sex\"] = label_encoder.fit_transform(test_data[\"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39eff67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived      int64\n",
      "Pclass        int64\n",
      "Sex           int32\n",
      "Age         float64\n",
      "SibSp         int64\n",
      "Parch         int64\n",
      "Fare        float64\n",
      "Embarked      int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0579f76f-7b83-4ec0-ae94-12367634cfe1",
   "metadata": {},
   "source": [
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(train_data.corr(), annot=True, linewidths=1, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9c06f59-ce51-498f-9e32-90c9f731db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Model\n",
    "target= train_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace9e8f3-e6a7-4bcc-83fb-3cbe0b718bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['Survived'], inplace = True, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f571654c-f2cc-4813-a270-6dba2e00d290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: X_train_with_targer=(702, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, target, test_size = 0.2, random_state=42, shuffle=True)\n",
    "#random_state 42 -> 상징적인 숫자, 알필요는 없음\n",
    "\n",
    "\n",
    "X_train_with_target = X_train.copy()\n",
    "#y_train['Survived'] = target # target -> \"DataFrame\"\n",
    "\n",
    "print(f'Sizes: X_train_with_targer={X_train_with_target.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cb0642-e506-40d4-a75f-e09e314f78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_model_name = None\n",
    "best_model_acc = 0.0\n",
    "\n",
    "models=[]\n",
    "\n",
    "def validate_model(model_name, model, accuracy):\n",
    "    global best_model, best_model_name, best_model_acc, models #밖에 있는 변수와 같은거? 혹은 다른거?\n",
    "    models.append([model_name, accuracy])\n",
    "\n",
    "    print()\n",
    "    print(f\"Current accuracy of model {model_name}: {accuracy}\")\n",
    "    print(f\"Previous best accuracy of model {best_model_name}: {best_model_acc}\")\n",
    "\n",
    "    if accuracy > best_model_acc:\n",
    "\n",
    "        #새로운 모델이 이전의  acc 갱신 시 best model 교체\n",
    "        print(f\"Improved previous accuracy!\")\n",
    "        best_model_acc = accuracy\n",
    "        best_model = model\n",
    "        best_model_name = model_name\n",
    "    else:\n",
    "        print(f\"Did not improve previous accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccff8b38-eec8-4b0c-9643-55706efbfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost\n",
    "# %%time \n",
    "# 매직 커맨드 time : 컴퓨터 내부에서 실행 시간, 컴퓨터 외부 실행시간(사용자가 바라보는 사용시간) 출력\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.captureWarnings(True) #에러 발생시 로그 생성\n",
    "simplefilter(action = 'ignore', category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d39a2a2-ddac-416c-8220-9f1a0af4960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current accuracy of model xgboost: 0.8238636363636364\n",
      "Previous best accuracy of model None: 0.0\n",
      "Improved previous accuracy!\n"
     ]
    }
   ],
   "source": [
    "train = xgb.DMatrix(data = X_train, label=y_train)\n",
    "test = xgb.DMatrix(data = X_test, label=y_test)\n",
    "xgboost_model = XGBClassifier(tree_method = 'gpu_hist')\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgboost =  xgboost_model.predict(X_test)\n",
    "accuracy =  accuracy_score(y_test, y_pred_xgboost)\n",
    "validate_model('xgboost',xgboost_model,accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6747f1ea-90ce-4603-bb29-0fbe3c732363",
   "metadata": {},
   "source": [
    "# Auto sklearn 파트에 쓰여있으나 이게 뭔지 모르겠음\n",
    "#%% capture\n",
    "#%% bash\n",
    "# https://github.com/automl/auto-sklearn/issues/101\n",
    "#apt-get remove swig\n",
    "#apt-get install swig3.0\n",
    "#ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "!pip3 install pyrfr\n",
    "# https://stackoverflow.com/questions/55833509/attributeerror-type-object-callable-has-no-attribute-abc-registry\n",
    "!pip3 uninstall -y typing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adc29d5d-a0ca-40a4-bd30-2a3653a83f41",
   "metadata": {},
   "source": [
    "%%capture\n",
    "%%bash\n",
    "# actual installation\n",
    "# curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip3 install\n",
    "pip3 install auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f50360e8-fcf5-40a0-905d-cfe3bdc2c278",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TPOT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#tpot\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTPOT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TPOTClassifier\n\u001b[0;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m logging\u001b[38;5;241m.\u001b[39mcaptureWarnings(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TPOT'"
     ]
    }
   ],
   "source": [
    "#tpot\n",
    "from TPOT import TPOTClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.captureWarnings(True)\n",
    "simplefilter(action = 'ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83137ca6-7629-410e-9f3a-61582535af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_classifier =  TPOTClassifier(generations=50, population_size=50, verbosity=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1033e192-92b8-4a21-a2c0-f55280604059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04093a709eb8419287545c27b0e168c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2550 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\32210182\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\32210182\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\32210182\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\__init__.py\", line 7, in <module>\n    from . import collective, dask, rabit\n  File \"C:\\Users\\32210182\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\collective.py\", line 12, in <module>\n    from .core import _LIB, _check_call, c_str, py_str, from_pystr_to_cstr\n  File \"C:\\Users\\32210182\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py\", line 264, in <module>\n    _LIB = _load_lib()\n           ^^^^^^^^^^^\n  File \"C:\\Users\\32210182\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py\", line 216, in _load_lib\n    raise XGBoostError(\nxgboost.core.XGBoostError: \nXGBoost Library (xgboost.dll) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"Could not find module 'C:\\\\Users\\\\32210182\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\xgboost\\\\lib\\\\xgboost.dll' (or one of its dependencies). Try using the full path with constructor syntax.\"]\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tpot\\base.py:817\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[1;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[0;32m    816\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 817\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pop, _ \u001b[38;5;241m=\u001b[39m \u001b[43meaMuPlusLambda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpopulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoolbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_toolbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulation_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcxpb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrossover_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmutpb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutation_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m            \u001b[49m\u001b[43mngen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhalloffame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pareto_front\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mper_generation_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_periodic_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_file_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;66;03m# Allow for certain exceptions to signal a premature fit() cancellation\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tpot\\gp_deap.py:232\u001b[0m, in \u001b[0;36meaMuPlusLambda\u001b[1;34m(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function, log_file)\u001b[0m\n\u001b[0;32m    230\u001b[0m     initialize_stats_dict(ind)\n\u001b[1;32m--> 232\u001b[0m population[:] \u001b[38;5;241m=\u001b[39m \u001b[43mtoolbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m record \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mcompile(population) \u001b[38;5;28;01mif\u001b[39;00m stats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tpot\\base.py:1574\u001b[0m, in \u001b[0;36mTPOTBase._evaluate_individuals\u001b[1;34m(self, population, features, target, sample_weight, groups)\u001b[0m\n\u001b[0;32m   1571\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m   1572\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2*n_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1573\u001b[0m     )\n\u001b[1;32m-> 1574\u001b[0m     tmp_result_scores \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_wrapped_cross_val_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m            \u001b[49m\u001b[43msklearn_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msklearn_pipeline\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msklearn_pipeline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msklearn_pipeline_list\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[38;5;66;03m# update pbar\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtpot_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tpot\\base.py:864\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[1;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# raise the exception if it's our last attempt\u001b[39;00m\n\u001b[0;32m    863\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m attempt \u001b[38;5;241m==\u001b[39m (attempts \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 864\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tpot\\base.py:855\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[1;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pbar, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pbar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 855\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_top_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary_of_best_pipeline(features, target)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# Delete the temporary cache before exiting\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tpot\\base.py:962\u001b[0m, in \u001b[0;36mTPOTBase._update_top_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    958\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_optimized_pareto_front_n_gens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;66;03m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001b[39;00m\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# need raise RuntimeError because no pipeline has been optimized\u001b[39;00m\n\u001b[1;32m--> 962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA pipeline has not yet been optimized. Please call fit() first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    964\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "tpot_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d5329-c457-421d-a101-b99ad92d44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_tpot = tpot_classifier.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
